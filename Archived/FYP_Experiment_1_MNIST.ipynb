{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "EnzyDhFcryu-",
    "outputId": "86fec1d2-6cd8-4785-a655-0abcc78ad01b"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import skimage\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from skimage.measure import compare_ssim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Jia Lin Gao 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is adapted from blockquoted tutorial below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dw79PqmAryvH"
   },
   "source": [
    "\n",
    ">What is `torch.nn` *really*?\n",
    "============================\n",
    "by Jeremy Howard, `fast.ai <https://www.fast.ai>`_. Thanks to Rachel Thomas and Francisco Ingham.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-imu9jYFryvO"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.getcwd()+\"/data\"\n",
    "PATH = Path(DATA_PATH+\"/mnist\")\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXsyHWlTryvT"
   },
   "source": [
    ">This dataset is in numpy array format, and has been stored using pickle,\n",
    "a python-specific format for serializing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OdIvjDaryvV"
   },
   "outputs": [],
   "source": [
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJTytUHKryve"
   },
   "source": [
    ">Each image is 28 x 28, and is being stored as a flattened row of length\n",
    "784 (=28x28). Let's take a look at one; we need to reshape it to 2d\n",
    "first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "4RQxBoaoryvf",
    "outputId": "d80fc41c-fc47-4e76-af1c-ce54daf2fd56"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-22477f5f7133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mshowTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-22477f5f7133>\u001b[0m in \u001b[0;36mshowTensor\u001b[0;34m(x, dim, idx)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mshowTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "def showTensor(x, dim = 28, idx = 0):\n",
    "    if x.shape[1]==dim:\n",
    "        pyplot.imshow(x.numpy(), cmap=\"gray\")\n",
    "    else:\n",
    "        pyplot.imshow(x[idx].detach().numpy().reshape((dim,dim)), cmap='gray')\n",
    "    \n",
    "showTensor(x_train, 28)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now play with some matrix and image transformations using PIL and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "K-zwvlI9RFxW",
    "outputId": "5a1fdec3-24e7-4a85-c09b-827b8b074e81"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "e1 = x_train[0].reshape((28,28))\n",
    "showTensor(e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNrZnj9HJmko"
   },
   "source": [
    "Lets convert it into unsigned int form so we can then convert them to PIL format and use the PIL library for image transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "Dx_e6t8ZUf_U",
    "outputId": "941425a7-a9a4-4cbf-e3a8-95593f6b79ca"
   },
   "outputs": [],
   "source": [
    "e2 = np.uint8(e1*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOG7NdYsJ15d"
   },
   "source": [
    "Pil does the downsampling for us wew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REYvInneU0zv"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-87bda96a0387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0me4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshowTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-cbac47334c2b>\u001b[0m in \u001b[0;36mshowTensor\u001b[0;34m(x, dim, idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshowTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'numpy'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAMhUlEQVR4nO3df6hX9R3H8dcrtTINyw3EvP36w4ooqyFhtdrQFq4i98egYo2aAwncljFYRlDsv8FkFDTLS+VqWf1hrSJwy7VFLFZoP2imrZxzadNs2DKSsEvv/fE9gt3pVc7nnHO/+X4+4HK/53zP+74/9+LL8+P7Pd+PI0IADn9HjPYAAHSDsANJEHYgCcIOJEHYgSTGdtnMNpf+gZZFhPe3nj07kARhB5Ig7EAShB1Ioijstufa/rvtjbYXNzUoAM1z3ffG2x4j6W1J35K0VdIaSddGxPoRargaD7Ssjavx50vaGBGbImKPpMckzSv4eQBaVBL2aZK27LO8tVr3BbYX2F5re21BLwCFWn9TTUQMShqUOIwHRlPJnv09SSfuszxQrQPQh0rCvkbSdNun2j5S0jWSnm5mWACaVvswPiKGbP9I0h8kjZH0QES82djIADSq9ktvtZpxzg60jhthgOQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkOp2yGfXY+/3gkUMyadKkot4TJ04sqj/nnHNq1+7evbuo97nnnlu7dufOnUW9H3roodq1bX16FHt2IAnCDiRB2IEkCDuQRO2w2z7R9p9tr7f9pu2bmhwYgGaVXI0fkvTTiHjV9rGSXrG9eqQpmwGMntp79ojYFhGvVo8/lrRB+5nFFUB/aOR1dtunSDpP0sv7eW6BpAVN9AFQX3HYbU+U9LikRRGxa/jzTNkM9Ieiq/G2x6kX9BUR8UQzQwLQhpKr8ZZ0v6QNEfGr5oYEoA0le/aLJH1f0mzbr1dflzc0LgANK5mf/S+S6t+hAaBTvIMOSIKwA0lwP/shmjx5cu3axYsXF/UuuSf8jDPOKOo9derUovpx48bVrr377ruLepfci79q1aqi3m3dk16CPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJbnE9RENDQ7VrTz755KLeZ599du3ao446qqj3LbfcUlQ/ffr02rW33357Ue8PP/ywqP5ww54dSIKwA0kQdiAJwg4kURx222Nsv2b7mSYGBKAdTezZb1JvBlcAfax0rrcBSVdIuq+Z4QBoS+me/U5JP5P0+YE2sL3A9lrbawt7AShQMrHjlZJ2RMQrI20XEYMRMTMiZtbtBaBc6cSOV9neLOkx9SZ4fLiRUQFoXO2wR8StETEQEadIukbSnyLiusZGBqBRvM4OJNHIjTAR8byk55v4WQDawZ4dSIKwA0m4y6llbfffPLYdsF1Uf/XVV9euXbhwYVHviy++uKge3YuI/f6DY88OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgltcvwQmTJhQu3b58uVFvTdv3lxUv3Tp0lHrnRW3uALJEXYgCcIOJEHYgSRKJ3Y8zvZK22/Z3mD7gqYGBqBZpZ8bf5ek30fEd20fKemYBsYEoAW1w257kqRLJN0gSRGxR9KeZoYFoGklh/GnSvpA0nLbr9m+z/b/vSDMlM1AfygJ+1hJX5N0T0ScJ+kTSYuHb8SUzUB/KAn7VklbI+LlanmleuEH0IdKpmzeLmmL7dOrVXMkrW9kVAAaV3o1/seSVlRX4jdJ+kH5kAC0oSjsEfG6JM7FgS8B3kEHJEHYgSS4n/0wNzAwUFS/bNmyovqTTjqpdu2NN95Y1PvFF18sqv+y4n52IDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIL72TGiI44o2x888sgjtWtnz55d1HvGjBm1a7dv317UezRxPzuQHGEHkiDsQBKlUzbfbPtN2+tsP2r76KYGBqBZtcNue5qkn0iaGRFnSRoj6ZqmBgagWaWH8WMljbc9Vr252f9dPiQAbSiZ6+09SUskvStpm6SPIuLZ4dsxZTPQH0oO44+XNE+9edpPkDTB9nXDt2PKZqA/lBzGXyrpnxHxQUR8JukJSRc2MywATSsJ+7uSZtk+xrbVm7J5QzPDAtC0knP2lyWtlPSqpL9VP2uwoXEBaFjplM13SLqjobEAaBHvoAOSIOxAEkWH8eh/s2bNKqpfuHBhUf3cuXNr127ZsqWo9+7du4vqDzfs2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJ7mfvwGmnnVZUv2jRotq18+fPL+r96aefFtUvW7asdu3SpUuLeu/atauo/nDDnh1IgrADSRB2IImDht32A7Z32F63z7rJtlfbfqf6fny7wwRQ6lD27L+RNPxTAxdLei4ipkt6rloG0McOGvaIeEHSzmGr50l6sHr8oKTvNDwuAA2r+9LblIjYVj3eLmnKgTa0vUDSgpp9ADSk+HX2iAjbMcLzg6rmgBtpOwDtqns1/n3bUyWp+r6juSEBaEPdsD8t6frq8fWSnmpmOADacigvvT0q6a+STre91fYPJf1C0rdsvyPp0moZQB876Dl7RFx7gKfmNDwWAC3iHXRAEoQdSCLNLa6XXXZZUX3J1MWzZ88u6j1+/PjatYODg0W9lyxZUlS/adOmono0hz07kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJPGlup/ddu3aKVMO+NH2h2T9+vW1a5988smi3mvWrKldu27duoNvhBTYswNJEHYgCcIOJFF3yuZf2n7L9hu2f2f7uHaHCaBU3SmbV0s6KyJmSHpb0q0NjwtAw2pN2RwRz0bEULX4kqSBFsYGoEFNnLPPl7SqgZ8DoEVFr7Pbvk3SkKQVI2zD/OxAH6gddts3SLpS0pyIYH52oM/VCrvtuZJ+JukbEbG72SEBaEPdKZvvlnSspNW2X7d9b8vjBFCo7pTN97cwFgAt4h10QBKEHUjCI1xIb74ZV+OB1kXEfu8FZ88OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXQ9ZfN/JP1rhOe/Wm0zGuhN78Oh98kHeqLTD684GNtrI2ImvelN7+ZxGA8kQdiBJPot7IP0pje929FX5+wA2tNve3YALSHsQBJ9EXbbc23/3fZG24s77Hui7T/bXm/7Tds3ddV7nzGMsf2a7Wc67nuc7ZW237K9wfYFHfa+ufp7r7P9qO2jW+73gO0dttfts26y7dW236m+H99h719Wf/c3bP/O9nFt9B5u1MNue4ykX0v6tqQzJV1r+8yO2g9J+mlEnClplqSFHfbe6yZJGzruKUl3Sfp9RJwh6ZyuxmB7mqSfSJoZEWdJGiPpmpbb/kbS3GHrFkt6LiKmS3quWu6q92pJZ0XEDElvS7q1pd5fMOphl3S+pI0RsSki9kh6TNK8LhpHxLaIeLV6/LF6/+CnddFbkmwPSLpC0n1d9az6TpJ0iaoJOiNiT0T8t8MhjJU03vZYScdI+nebzSLiBUk7h62eJ+nB6vGDkr7TVe+IeDYihqrFlyQNtNF7uH4I+zRJW/ZZ3qoOA7eX7VMknSfp5Q7b3qnePPefd9hTkk6V9IGk5dUpxH22J3TROCLek7RE0ruStkn6KCKe7aL3MFMiYlv1eLukKaMwBkmaL2lVF436IeyjzvZESY9LWhQRuzrqeaWkHRHxShf9hhkr6WuS7omI8yR9ovYOY7+gOjeep95/OCdImmD7ui56H0j0Xn/u/DVo27epdyq5oot+/RD29ySduM/yQLWuE7bHqRf0FRHxRFd9JV0k6Srbm9U7dZlt++GOem+VtDUi9h7FrFQv/F24VNI/I+KDiPhM0hOSLuyo977etz1VkqrvO7psbvsGSVdK+l509GaXfgj7GknTbZ9q+0j1LtY83UVj21bvvHVDRPyqi557RcStETEQEaeo9zv/KSI62cNFxHZJW2yfXq2aI2l9F73VO3yfZfuY6u8/R6NzgfJpSddXj6+X9FRXjW3PVe/07aqI2N1VX0XEqH9July9q5L/kHRbh32/rt7h2xuSXq++Lh+F3/+bkp7puOe5ktZWv/uTko7vsPfPJb0laZ2k30o6quV+j6p3feAz9Y5qfijpK+pdhX9H0h8lTe6w90b1rlPt/Td3bxd/d94uCyTRD4fxADpA2IEkCDuQBGEHkiDsQBKEHUiCsANJ/A/8npaqMQgirwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "e3 = PIL.Image.fromarray(e2)\n",
    "e4 = e3.resize((14, 14))\n",
    "pyplot.imshow(e4, cmap=\"gray\")\n",
    "showTensor(e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJeSwMAzJ8rp"
   },
   "source": [
    "The PIL library can also upsample for us which is convenient - no need to do it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "_fvC3WX6RzQv",
    "outputId": "4080c983-4796-4232-e2e5-27ae0e1d31c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3eec98c2e8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "e5 = e4.resize((28,28),resample = 3)\n",
    "pyplot.imshow(e5, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate mean absolute error, mean squared error and structural similarity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Pr9zPN8kZFcc",
    "outputId": "1b1b19bc-2122-4f80-c067-2f733ec26b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of bicubic interpolation on this image is 40.17602040816327\n",
      "The SSIM of bicubic interpolation on this image is 0.839661764924514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/qg35/jlgao2/venvs/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "e5_mae = np.average(np.asarray(e5) - np.asarray(e3))\n",
    "e5_mse = np.average(np.square(np.asarray(e5)-np.asarray(e3))) \n",
    "\n",
    "print(\"The MSE of bicubic interpolation on this image is\", e5_mse)\n",
    "e5_ssim = ssim(np.asarray(e5), np.asarray(e3))\n",
    "print(\"The SSIM of bicubic interpolation on this image is\", e5_ssim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjwREgqpKAy2"
   },
   "source": [
    "We've validated the approach, now lets downsample the whole dataset\n",
    "\n",
    "wew I just did a python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCDTB0VkKUYn"
   },
   "source": [
    "We can use much of the same code to obtain upsampleed images using bicubic or bilinear interpolations to measure the network that we will surely build ^__^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QaAPo7g0lU9"
   },
   "outputs": [],
   "source": [
    "def downupsample(array):\n",
    "  dsda = np.ones((array.shape[0],196)) \n",
    "  usda = np.ones((array.shape[0],784)) \n",
    "\n",
    "  for i in range(array.shape[0]):\n",
    "    ar = array[i].reshape((28,28))\n",
    "    ar = np.int8(ar*255)\n",
    "    ar2 = PIL.Image.fromarray(ar)\n",
    "    ar3 = ar2.resize((14,14))\n",
    "    ar4 = np.asarray(ar3).reshape((1, 196))\n",
    "    ar5 = ar4.astype(np.float64)/255\n",
    "    dsda[i,:] = np.clip(ar5,0,1)\n",
    "    ar6 = ar3.resize((28,28), resample = 3)\n",
    "    ar7 = np.asarray(ar6).reshape((1, 784))\n",
    "    ar8 = ar7.astype(np.float32)/255\n",
    "    usda[i,:] = np.clip(ar8,0,1)\n",
    "  return dsda, usda\n",
    "xt_ds, xt_us = downupsample(x_train)\n",
    "xv_ds, xv_us= downupsample(x_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "wLotz1h2ryvl",
    "outputId": "cb0f0eb6-5b47-4f78-cb11-ac6591fcea19",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/qg35/jlgao2/venvs/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/projects/qg35/jlgao2/venvs/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x_train, xt_ds, xt_us = map(\n",
    "torch.tensor, (x_train, xt_ds, xt_us)\n",
    "    )\n",
    "\n",
    "x_valid, xv_ds, xv_us = map(\n",
    "torch.tensor, (x_valid, xv_ds, xv_us)\n",
    "    )\n",
    "\n",
    "[n, c] = x_train.shape\n",
    "\n",
    "x_train = x_train.float()\n",
    "xt_ds = xt_ds.float()\n",
    "xt_us = xt_us.float()\n",
    "x_valid = x_valid.float()\n",
    "xv_ds = xv_ds.float()\n",
    "xv_us = xv_us.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "YbFlN9Q2szAi",
    "outputId": "32b8508e-ce9f-469f-9914-dc020727f7fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010889884\n",
      "0.8491649687219552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/qg35/jlgao2/venvs/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "ssim = []\n",
    "\n",
    "for i in range(100):\n",
    "    mse.append(np.average(np.square(np.asarray(xt_us[i])-np.asarray(x_train[i]))))\n",
    "    ssim.append(compare_ssim(np.asarray(xt_us[i]), np.asarray(x_train[i])))\n",
    "\n",
    "#     e5_mse =  \n",
    "# from skimage.measure import compare_ssim as ssim\n",
    "# print(\"The MSE of bicubic interpolation on this image is\", e5_mse)\n",
    "# e5_ssim = \n",
    "# print(\"The SSIM of bicubic interpolation on this image is\", e5_ssim)\n",
    "\n",
    "print(np.mean(mse))\n",
    "print(np.mean(ssim))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jE1eLYgvsywZ"
   },
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-VKlkVdryvu"
   },
   "source": [
    ">Neural net from scratch (no torch.nn)\n",
    "---------------------------------------------\n",
    "\n",
    ">Let's first create a model using nothing but PyTorch tensor operations. We're assuming\n",
    "you're already familiar with the basics of neural networks. (If you're not, you can\n",
    "learn them at `course.fast.ai <https://course.fast.ai>`_).\n",
    "\n",
    ">PyTorch provides methods to create random or zero-filled tensors, which we will\n",
    "use to create our weights and bias for a simple linear model. These are just regular\n",
    "tensors, with one very special addition: we tell PyTorch that they require a\n",
    "gradient. This causes PyTorch to record all of the operations done on the tensor,\n",
    "so that it can calculate the gradient during back-propagation *automatically*!\n",
    "\n",
    ">For the weights, we set ``requires_grad`` **after** the initialization, since we\n",
    "don't want that step included in the gradient. (Note that a trailling ``_`` in\n",
    "PyTorch signifies that the operation is performed in-place.)\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>We are initializing the weights here with\n",
    "   `Xavier initialisation <http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>`_\n",
    "   (by multiplying with 1/sqrt(n)).</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Llsgi-Lwryvw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(196, 784) / 196\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(784, requires_grad=True)\n",
    "\n",
    "xt_ds.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXsxpMuKryv0"
   },
   "source": [
    ">Thanks to PyTorch's ability to calculate gradients automatically, we can\n",
    "use any standard Python function (or callable object) as a model! So\n",
    "let's just write a plain matrix multiplication and broadcasted addition\n",
    "to create a simple linear model. We also need an activation function, so\n",
    "we'll write `log_softmax` and use it. Remember: although PyTorch\n",
    "provides lots of pre-written loss functions, activation functions, and\n",
    "so forth, you can easily write your own using plain python. PyTorch will\n",
    "even create fast GPU or vectorized CPU code for your function\n",
    "automatically.\n",
    "\n",
    "For a regression problem we cannot use a log softmax, so we modify this model to use a Relu activation function. Jeremy Howard says it works better for classification problems anyway\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "288zx75Iryv1"
   },
   "outputs": [],
   "source": [
    "def relu(x): #activation function\n",
    "    return torch.clamp(x, 0, 1)\n",
    "\n",
    "def model(xb): \n",
    "    return relu(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F35xmdJwryv5"
   },
   "source": [
    ">In the above, the ``@`` stands for the dot product operation. We will call\n",
    "our function on one batch of data (in this case, 64 images).  This is\n",
    "one *forward pass*.  Note that our predictions won't be any better than\n",
    "random at this stage, since we start with random weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHfdVe0nryv6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.9451e-02, 4.4994e-02, 0.0000e+00, 1.5130e-02, 1.8985e-02, 0.0000e+00,\n",
      "        2.0024e-02, 2.1164e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        5.4662e-03, 0.0000e+00, 0.0000e+00, 1.9227e-02, 0.0000e+00, 6.0804e-03,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0985e-02, 0.0000e+00, 1.2064e-02,\n",
      "        7.2111e-03, 1.8518e-02, 2.7636e-02, 0.0000e+00, 1.9305e-03, 3.5012e-02,\n",
      "        2.1956e-02, 0.0000e+00, 3.7554e-02, 0.0000e+00, 0.0000e+00, 2.6167e-02,\n",
      "        0.0000e+00, 1.9710e-02, 8.7647e-03, 0.0000e+00, 2.5703e-02, 9.9076e-03,\n",
      "        1.8519e-02, 0.0000e+00, 6.0988e-02, 1.1830e-02, 0.0000e+00, 0.0000e+00,\n",
      "        2.0182e-03, 3.1915e-02, 1.0914e-02, 6.3087e-03, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0874e-02, 0.0000e+00, 2.5244e-02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        3.5343e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0243e-02, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.2911e-02, 6.8407e-02, 0.0000e+00, 0.0000e+00,\n",
      "        1.3558e-02, 8.8630e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.4822e-03,\n",
      "        5.0821e-03, 0.0000e+00, 1.2693e-03, 2.0745e-02, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 2.9945e-03, 0.0000e+00, 8.8706e-03, 2.7267e-02, 0.0000e+00,\n",
      "        2.6034e-02, 8.3103e-03, 0.0000e+00, 2.5393e-02, 2.9679e-02, 1.7712e-03,\n",
      "        2.1928e-03, 0.0000e+00, 4.2747e-02, 2.2807e-02, 1.3673e-02, 6.8406e-03,\n",
      "        0.0000e+00, 0.0000e+00, 2.5645e-02, 4.4512e-03, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 2.8088e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9636e-02,\n",
      "        0.0000e+00, 8.7909e-03, 0.0000e+00, 9.2851e-03, 0.0000e+00, 2.2166e-02,\n",
      "        0.0000e+00, 0.0000e+00, 1.4239e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        7.7130e-03, 0.0000e+00, 3.9777e-02, 2.2517e-02, 3.3885e-02, 0.0000e+00,\n",
      "        1.8065e-03, 6.2844e-03, 1.8806e-02, 1.3611e-02, 3.4080e-02, 2.8929e-02,\n",
      "        0.0000e+00, 0.0000e+00, 1.2753e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.7165e-02, 0.0000e+00, 0.0000e+00, 4.3790e-02,\n",
      "        5.9093e-03, 0.0000e+00, 9.9401e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        9.9771e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5318e-03,\n",
      "        1.4915e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2052e-04, 0.0000e+00,\n",
      "        2.7839e-04, 1.1793e-02, 0.0000e+00, 0.0000e+00, 4.2514e-03, 0.0000e+00,\n",
      "        0.0000e+00, 1.4770e-02, 9.4719e-03, 2.4087e-02, 0.0000e+00, 0.0000e+00,\n",
      "        2.1479e-02, 2.7548e-02, 0.0000e+00, 3.1993e-02, 0.0000e+00, 0.0000e+00,\n",
      "        6.5583e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3401e-03,\n",
      "        0.0000e+00, 0.0000e+00, 2.8849e-03, 1.8797e-02, 2.7971e-03, 0.0000e+00,\n",
      "        0.0000e+00, 3.3509e-03, 3.3390e-02, 0.0000e+00, 1.9971e-02, 2.6373e-02,\n",
      "        8.3248e-03, 0.0000e+00, 0.0000e+00, 4.7285e-02, 0.0000e+00, 0.0000e+00,\n",
      "        1.6213e-02, 2.6060e-03, 1.6059e-02, 6.4566e-03, 1.8046e-02, 1.4203e-03,\n",
      "        0.0000e+00, 0.0000e+00, 8.4880e-03, 2.4456e-02, 1.3174e-02, 7.4450e-03,\n",
      "        0.0000e+00, 1.7674e-02, 0.0000e+00, 0.0000e+00, 3.8881e-03, 0.0000e+00,\n",
      "        0.0000e+00, 3.4694e-02, 0.0000e+00, 0.0000e+00, 2.6041e-02, 0.0000e+00,\n",
      "        6.0661e-03, 0.0000e+00, 7.7585e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1238e-02, 7.8301e-03, 5.8383e-03,\n",
      "        4.4252e-03, 2.9099e-02, 0.0000e+00, 5.6875e-03, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2140e-03, 8.9846e-04, 0.0000e+00,\n",
      "        0.0000e+00, 2.4892e-03, 2.9636e-04, 1.9289e-02, 0.0000e+00, 5.8458e-03,\n",
      "        9.4640e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0701e-02, 1.5759e-02,\n",
      "        0.0000e+00, 9.4482e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 6.2873e-03, 3.4575e-05, 1.8760e-03, 5.9702e-02, 0.0000e+00,\n",
      "        1.4966e-03, 0.0000e+00, 1.3324e-02, 1.6120e-02, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0222e-02, 1.8712e-03, 0.0000e+00, 2.8874e-02, 1.5470e-02,\n",
      "        0.0000e+00, 0.0000e+00, 1.7393e-02, 4.0390e-02, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 6.4479e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        3.6738e-04, 6.3188e-03, 0.0000e+00, 3.2563e-03, 8.2416e-03, 0.0000e+00,\n",
      "        3.5189e-04, 0.0000e+00, 2.1966e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.4303e-02, 0.0000e+00, 2.7250e-02, 0.0000e+00, 0.0000e+00, 8.6177e-03,\n",
      "        1.4413e-02, 1.8804e-02, 3.0493e-02, 2.5179e-02, 0.0000e+00, 1.5937e-02,\n",
      "        1.6033e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3472e-02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4646e-02, 3.3880e-02, 3.4016e-02,\n",
      "        1.3456e-02, 0.0000e+00, 2.8986e-03, 2.7867e-02, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 2.6257e-04, 0.0000e+00, 4.1689e-02, 1.2617e-02, 1.3339e-03,\n",
      "        1.0315e-02, 2.6870e-02, 1.2041e-02, 4.7000e-03, 1.6873e-03, 0.0000e+00,\n",
      "        0.0000e+00, 3.6373e-02, 5.4411e-03, 6.6000e-03, 0.0000e+00, 2.7483e-03,\n",
      "        0.0000e+00, 3.0767e-02, 7.8759e-03, 0.0000e+00, 4.1241e-02, 2.1866e-02,\n",
      "        1.7476e-02, 0.0000e+00, 3.0751e-03, 0.0000e+00, 0.0000e+00, 2.4198e-03,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5516e-02, 0.0000e+00, 1.2039e-03,\n",
      "        8.0386e-05, 7.7617e-03, 1.8059e-02, 0.0000e+00, 0.0000e+00, 3.0911e-02,\n",
      "        2.5978e-02, 3.0537e-02, 3.3396e-02, 0.0000e+00, 0.0000e+00, 2.1550e-02,\n",
      "        1.8193e-02, 5.0127e-02, 9.9611e-03, 0.0000e+00, 2.7125e-02, 2.9712e-02,\n",
      "        3.4957e-02, 7.5532e-03, 0.0000e+00, 2.4743e-02, 0.0000e+00, 1.1655e-02,\n",
      "        3.8427e-02, 5.1783e-03, 0.0000e+00, 1.2445e-02, 2.1681e-02, 0.0000e+00,\n",
      "        3.4665e-02, 0.0000e+00, 0.0000e+00, 2.9553e-03, 2.3587e-02, 0.0000e+00,\n",
      "        6.5250e-03, 4.0534e-03, 9.8774e-03, 0.0000e+00, 1.9572e-02, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8606e-04, 0.0000e+00,\n",
      "        1.2690e-02, 1.6973e-02, 0.0000e+00, 4.4127e-02, 3.2556e-02, 7.3859e-04,\n",
      "        0.0000e+00, 0.0000e+00, 1.2662e-03, 0.0000e+00, 2.6262e-02, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.3320e-03, 6.4796e-03, 0.0000e+00, 5.0846e-03,\n",
      "        2.7723e-02, 1.5953e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        7.1615e-03, 5.3924e-04, 0.0000e+00, 1.6208e-03, 2.0500e-02, 1.9590e-02,\n",
      "        0.0000e+00, 0.0000e+00, 2.3461e-03, 0.0000e+00, 0.0000e+00, 2.2184e-02,\n",
      "        1.6747e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.0288e-03,\n",
      "        1.9273e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.4677e-03, 6.1949e-02,\n",
      "        0.0000e+00, 2.1596e-02, 1.3106e-02, 0.0000e+00, 5.4853e-03, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 3.4151e-03, 0.0000e+00, 2.9415e-02, 2.7701e-02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4517e-02, 0.0000e+00, 0.0000e+00,\n",
      "        3.0658e-02, 0.0000e+00, 2.7946e-02, 4.9078e-02, 2.0990e-02, 1.3861e-03,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4697e-02, 1.9094e-02, 1.7637e-02,\n",
      "        1.0727e-02, 9.8851e-03, 0.0000e+00, 2.8415e-03, 5.4244e-02, 1.9421e-02,\n",
      "        2.8558e-02, 2.7009e-02, 0.0000e+00, 1.4192e-02, 0.0000e+00, 9.8058e-03,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5581e-02, 6.5617e-03, 0.0000e+00,\n",
      "        3.4558e-02, 0.0000e+00, 9.3185e-03, 0.0000e+00, 0.0000e+00, 4.6440e-03,\n",
      "        2.6977e-03, 1.2810e-02, 5.0489e-03, 0.0000e+00, 0.0000e+00, 2.0612e-02,\n",
      "        3.3778e-02, 2.4346e-02, 0.0000e+00, 6.0858e-03, 1.0287e-02, 1.1202e-02,\n",
      "        0.0000e+00, 0.0000e+00, 4.8028e-03, 0.0000e+00, 0.0000e+00, 1.0854e-02,\n",
      "        0.0000e+00, 0.0000e+00, 2.7856e-02, 0.0000e+00, 0.0000e+00, 1.8763e-02,\n",
      "        7.9071e-03, 8.3594e-03, 4.2409e-02, 5.9858e-03, 3.8555e-03, 0.0000e+00,\n",
      "        6.7782e-03, 0.0000e+00, 1.0420e-02, 0.0000e+00, 0.0000e+00, 8.1879e-03,\n",
      "        2.4007e-02, 1.6558e-02, 0.0000e+00, 2.0047e-02, 0.0000e+00, 1.0840e-02,\n",
      "        0.0000e+00, 3.9935e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1231e-03,\n",
      "        0.0000e+00, 1.1253e-02, 0.0000e+00, 7.2680e-04, 2.6119e-02, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6923e-02, 1.3974e-03,\n",
      "        0.0000e+00, 1.0586e-02, 2.1638e-02, 1.5337e-02, 3.6974e-04, 2.3960e-03,\n",
      "        5.8932e-03, 3.0077e-02, 1.9828e-02, 0.0000e+00, 0.0000e+00, 1.1681e-02,\n",
      "        2.5906e-02, 2.1915e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.5590e-03, 2.5830e-02, 0.0000e+00, 2.1659e-02, 8.7718e-03, 0.0000e+00,\n",
      "        2.1154e-02, 0.0000e+00, 0.0000e+00, 3.2764e-03, 3.1238e-04, 0.0000e+00,\n",
      "        1.7686e-02, 1.0220e-02, 1.3523e-02, 0.0000e+00, 1.1598e-02, 2.9122e-03,\n",
      "        0.0000e+00, 0.0000e+00, 2.8636e-02, 2.3426e-02, 4.8806e-04, 2.9141e-03,\n",
      "        3.8022e-03, 4.7784e-03, 0.0000e+00, 1.0549e-03, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0106e-03, 0.0000e+00, 8.0436e-03,\n",
      "        1.9957e-02, 2.8717e-03, 0.0000e+00, 0.0000e+00, 4.3507e-02, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 4.4814e-03, 2.7036e-02, 7.6757e-03, 2.5966e-04,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7467e-03,\n",
      "        0.0000e+00, 4.2101e-02, 1.3451e-03, 1.2361e-02, 2.9674e-03, 0.0000e+00,\n",
      "        4.8215e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9935e-03, 1.3500e-02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3568e-04, 5.7747e-02, 2.1392e-02,\n",
      "        4.5756e-02, 0.0000e+00, 0.0000e+00, 1.1525e-02, 3.5862e-02, 0.0000e+00,\n",
      "        1.2016e-02, 4.1200e-02, 0.0000e+00, 2.4883e-03, 1.3064e-03, 1.3282e-02,\n",
      "        0.0000e+00, 0.0000e+00, 2.4324e-02, 0.0000e+00, 0.0000e+00, 2.1337e-02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7070e-02, 0.0000e+00,\n",
      "        4.5480e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        2.3990e-02, 0.0000e+00, 4.3413e-02, 4.3777e-03, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3751e-02, 1.7686e-03,\n",
      "        2.4966e-03, 0.0000e+00, 7.7573e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0138e-02, 2.2617e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 5.0617e-02, 5.1056e-03, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.6144e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9895e-02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9638e-04,\n",
      "        0.0000e+00, 1.8101e-03, 0.0000e+00, 8.7484e-04, 0.0000e+00, 3.8494e-02,\n",
      "        0.0000e+00, 0.0000e+00, 1.9265e-02, 0.0000e+00, 3.8993e-02, 2.1805e-02,\n",
      "        3.3903e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SelectBackward>) tensor([ 1.3477e-03, -6.9730e-03,  3.4687e-03,  2.7482e-03,  8.8088e-03,\n",
      "         3.8709e-03,  1.1051e-02, -1.6449e-03, -2.2403e-03, -9.4582e-03,\n",
      "         6.1003e-03, -6.8457e-03,  5.6094e-03, -2.5407e-03, -1.1083e-02,\n",
      "        -2.9756e-03,  6.8697e-03, -5.5022e-03,  1.3378e-03,  6.8012e-03,\n",
      "         9.0285e-04, -3.3984e-03,  2.0889e-03, -3.6804e-03,  3.9407e-03,\n",
      "        -5.5826e-03,  2.3696e-03, -9.5690e-03, -7.3104e-03, -1.2550e-03,\n",
      "        -1.4364e-03,  1.1694e-02,  4.5194e-03,  1.0147e-02,  4.3105e-03,\n",
      "         1.0785e-03,  6.2917e-03,  2.7874e-03, -1.4490e-03,  7.2071e-03,\n",
      "         1.0952e-02,  6.0858e-03,  2.7890e-03,  1.0029e-02,  1.3757e-03,\n",
      "        -5.0073e-03, -5.9716e-03, -2.3325e-03, -5.2764e-04,  7.8837e-04,\n",
      "        -5.5671e-03,  7.1788e-04, -6.0835e-03,  6.1201e-04, -2.9801e-03,\n",
      "        -7.6422e-03,  4.9371e-03, -2.3326e-03,  7.3566e-03,  1.9583e-03,\n",
      "        -3.0261e-03, -1.5543e-04, -3.2950e-03,  4.9123e-03, -8.6790e-04,\n",
      "        -1.2312e-03, -1.5722e-03,  5.9622e-03,  1.4450e-03, -1.0350e-02,\n",
      "         3.6257e-03,  1.5457e-03, -4.2502e-03, -9.7257e-03, -1.1341e-03,\n",
      "         3.5733e-03, -1.0396e-02, -1.5436e-03,  2.7912e-03, -3.7892e-04,\n",
      "        -4.6095e-03, -2.7803e-04,  1.9844e-03, -5.7262e-03, -6.6511e-03,\n",
      "         5.4078e-03,  1.1126e-02,  1.1798e-03,  4.9001e-03,  4.2074e-03,\n",
      "         7.8439e-04,  3.5089e-03, -5.1925e-04,  2.2599e-03, -1.2510e-03,\n",
      "        -7.1698e-03, -6.9298e-03,  1.8520e-04,  1.1460e-02,  4.7775e-03,\n",
      "        -5.4946e-05,  2.3392e-03, -1.6301e-03, -5.6389e-03, -1.5939e-04,\n",
      "         1.7286e-03,  8.8530e-03, -2.4324e-03, -9.0625e-03,  7.1193e-03,\n",
      "        -5.1849e-03,  1.8011e-03, -1.8536e-03, -2.3966e-03,  2.6068e-03,\n",
      "        -8.2831e-04,  4.4171e-03,  1.0936e-03,  2.2811e-03,  8.7677e-03,\n",
      "         4.5068e-03,  5.7862e-03, -3.8003e-03,  4.3221e-03, -3.5055e-03,\n",
      "         3.2021e-04,  8.1847e-03, -2.5406e-03,  1.1623e-03,  2.2556e-04,\n",
      "        -4.5161e-03,  7.9044e-03, -6.2530e-03,  2.0640e-03, -1.6170e-03,\n",
      "        -3.0062e-04,  4.3745e-03, -1.0417e-03, -4.4161e-03, -5.2958e-03,\n",
      "         1.5056e-03,  2.9464e-03, -1.0663e-02,  3.1842e-03, -8.1862e-03,\n",
      "         5.8999e-03,  4.6711e-03,  2.8907e-03, -5.8095e-03, -2.6317e-05,\n",
      "        -1.9668e-03, -5.6324e-03, -5.4265e-03, -2.4963e-03, -7.3814e-03,\n",
      "        -4.0712e-03, -1.2946e-02, -7.6783e-03,  3.1869e-03,  3.9595e-03,\n",
      "         7.7097e-05,  3.0630e-03, -3.8116e-03,  8.0733e-03, -5.7101e-04,\n",
      "        -1.6394e-03,  2.4937e-03,  3.5013e-03,  2.7629e-03, -7.9942e-03,\n",
      "        -5.8632e-03, -4.7253e-03,  6.2047e-03, -4.1902e-03, -7.2085e-03,\n",
      "         2.1121e-03,  1.5091e-03,  5.2000e-03,  4.7760e-03, -1.4966e-04,\n",
      "        -1.1004e-03,  1.6412e-03,  2.5404e-03,  2.5988e-03,  7.6205e-03,\n",
      "         1.2090e-02,  1.3542e-03, -5.8578e-03, -6.7083e-03,  8.5894e-03,\n",
      "        -3.9638e-03, -1.0010e-02, -1.1867e-03, -3.8923e-03,  4.0309e-03,\n",
      "         7.3145e-03, -2.9257e-03, -1.1193e-03,  1.9022e-04, -4.2063e-03,\n",
      "         5.5650e-03, -3.0555e-03,  6.5945e-03,  2.2386e-03, -8.2684e-03,\n",
      "         1.4974e-03, -2.0183e-03,  8.2478e-04, -9.9198e-04, -7.4526e-03,\n",
      "        -2.8283e-03, -7.0087e-04,  9.5200e-03, -1.9149e-03,  6.5671e-03,\n",
      "         4.2536e-03,  1.2107e-02,  4.0466e-03, -4.7932e-03,  7.5468e-03,\n",
      "        -2.6120e-03,  2.3576e-03,  2.4790e-03,  2.4525e-03, -8.3366e-04,\n",
      "        -5.0666e-03,  2.0129e-03, -2.9309e-03,  3.1422e-03,  3.3380e-03,\n",
      "        -9.0674e-04, -5.3850e-03, -8.5950e-03, -3.9724e-03, -5.6690e-03,\n",
      "        -1.5357e-03,  1.0272e-02,  8.0534e-03,  6.4345e-03,  8.6847e-04,\n",
      "         3.4312e-04,  7.5896e-03, -1.5010e-03,  8.5872e-03,  3.2439e-03,\n",
      "        -8.1758e-03, -6.5249e-03,  5.6396e-03, -6.6897e-03, -1.1626e-02,\n",
      "         6.8516e-05, -4.7038e-03,  2.2464e-03, -3.3046e-03, -3.9715e-03,\n",
      "        -6.2249e-03, -2.1931e-03,  2.5124e-03,  2.4518e-03,  1.3467e-03,\n",
      "        -4.0388e-03, -3.6002e-03,  7.9802e-03, -4.4913e-03,  4.3948e-03,\n",
      "         1.5407e-02, -1.0382e-02, -3.5223e-03, -1.6727e-03, -3.3385e-03,\n",
      "        -6.7621e-03, -8.8602e-03,  6.7470e-03, -5.2807e-04,  5.7470e-03,\n",
      "        -3.6207e-03,  4.0289e-03,  3.6389e-03,  2.8731e-03, -2.1949e-03,\n",
      "        -3.3620e-03, -1.0805e-02,  6.2659e-03, -1.7976e-03, -4.8829e-03,\n",
      "        -5.6134e-03, -3.9198e-03,  6.9625e-03, -4.5219e-03,  2.4473e-03,\n",
      "        -1.8416e-03,  2.5592e-03,  1.6535e-03,  8.8958e-03,  3.6232e-04,\n",
      "         1.9992e-03,  6.2282e-03,  9.5035e-03, -5.7551e-03, -4.8874e-03,\n",
      "         9.7163e-03, -1.3342e-03,  5.5895e-03,  4.6323e-03,  1.0642e-02,\n",
      "         4.1888e-03, -1.9658e-03, -2.5133e-03, -3.1500e-03, -4.3732e-04,\n",
      "         2.6182e-03, -2.1913e-03,  7.2234e-03, -4.4828e-03, -7.0601e-03,\n",
      "        -5.6644e-03, -5.9603e-03, -1.1295e-02, -2.5075e-03, -2.0259e-03,\n",
      "         7.5948e-03,  4.7568e-03,  3.1450e-03,  4.8148e-04, -5.1234e-03,\n",
      "        -4.5007e-03, -6.9630e-03, -4.5286e-03, -5.1002e-03,  4.6041e-03,\n",
      "         6.9827e-03, -1.1855e-03, -6.9960e-03, -4.4938e-03,  1.7746e-03,\n",
      "         3.6291e-03,  3.1493e-03,  4.9855e-03, -5.9076e-03, -1.0783e-03,\n",
      "         6.4564e-03, -2.6240e-03,  1.0714e-02,  3.2856e-05, -3.6315e-03,\n",
      "         1.8578e-03, -2.6205e-03,  2.4018e-03, -5.9891e-03,  1.1504e-02,\n",
      "        -5.8167e-03,  1.0559e-02, -3.6027e-03, -7.1525e-04, -8.7684e-04,\n",
      "        -3.7751e-03, -1.8332e-03,  6.9884e-03, -7.5489e-03, -3.2399e-03,\n",
      "         3.2872e-03,  7.7746e-03,  9.4409e-04,  1.9985e-03,  1.4147e-03,\n",
      "         6.2624e-03,  1.9922e-03,  1.2587e-03, -3.1742e-03,  3.8018e-03,\n",
      "        -8.3691e-04,  8.5362e-03,  3.3800e-03,  4.4827e-03,  1.0808e-02,\n",
      "         2.4502e-03,  2.7996e-03,  3.8944e-03, -3.2036e-03,  7.1820e-03,\n",
      "         5.5694e-03,  8.7730e-03,  4.3114e-04,  3.9504e-03,  4.4384e-03,\n",
      "         5.9946e-03,  1.0780e-04, -1.6191e-03,  3.0874e-03, -4.6954e-03,\n",
      "        -3.2898e-03,  4.7054e-03,  3.5391e-03, -3.9565e-03, -6.0168e-03,\n",
      "         1.2577e-02,  4.2471e-03, -1.8346e-03, -2.4192e-03,  4.5062e-03,\n",
      "         2.4305e-03,  4.2387e-03,  7.8188e-03,  4.0672e-03,  3.8888e-03,\n",
      "         3.1115e-03, -1.4683e-03, -8.6096e-03,  3.2101e-03, -3.3175e-03,\n",
      "         2.9755e-03,  4.4635e-03, -1.6574e-04,  5.1473e-03, -1.0094e-02,\n",
      "        -7.6782e-03,  3.6558e-03, -6.8473e-03,  2.8290e-03,  1.2684e-03,\n",
      "         2.0766e-03,  3.6610e-03,  5.0291e-03, -2.6451e-03, -1.5576e-03,\n",
      "         4.7707e-03, -2.6656e-04, -6.0379e-04, -8.8659e-03, -1.6721e-03,\n",
      "        -2.0811e-04, -4.9743e-03, -2.5683e-03,  3.2823e-03,  1.9905e-03,\n",
      "         2.7207e-03, -1.9452e-03,  4.0074e-03, -9.8406e-03,  3.2662e-03,\n",
      "         4.8740e-03,  1.0986e-02,  3.9547e-03, -1.3858e-02,  6.7311e-03,\n",
      "         4.1684e-03, -4.8773e-03, -1.8608e-03, -2.4040e-03, -9.0241e-04,\n",
      "        -6.2788e-03,  4.3233e-03, -4.6239e-03,  2.8310e-03, -1.9684e-03,\n",
      "         5.8447e-03, -4.4640e-04,  2.0936e-03, -1.9640e-03,  4.0731e-03,\n",
      "        -6.9506e-03, -3.9727e-03,  3.9343e-03, -3.1710e-03, -5.6688e-04,\n",
      "         1.5269e-03,  1.8542e-03, -1.0032e-02,  8.5526e-03, -4.6005e-04,\n",
      "        -8.9390e-03, -3.3353e-04, -9.5264e-03, -6.3344e-04,  7.7191e-04,\n",
      "         1.7258e-03,  3.0122e-03, -5.3724e-03,  6.0093e-03, -2.9438e-03,\n",
      "        -1.0120e-03,  2.5263e-03,  2.9166e-03, -8.8378e-03, -5.3887e-03,\n",
      "         7.0878e-04,  2.6894e-03, -2.3817e-03,  8.0029e-03, -2.2581e-03,\n",
      "        -3.5983e-03,  1.1513e-03, -1.0521e-02,  7.1191e-03, -3.4230e-03,\n",
      "         2.3210e-03,  4.5692e-03,  5.2878e-03,  8.1849e-04, -1.0076e-03,\n",
      "        -8.5749e-03,  1.0876e-03,  2.3672e-03, -3.4707e-03,  3.2409e-03,\n",
      "         4.5227e-03,  2.8048e-03, -3.8126e-03,  2.5145e-03,  5.5663e-03,\n",
      "         1.1079e-02,  5.4390e-03, -5.4132e-03, -5.1450e-03,  7.2141e-03,\n",
      "         6.4292e-03,  4.7897e-03, -2.8690e-03,  3.8222e-03,  2.6175e-03,\n",
      "         2.6068e-03,  6.9078e-05, -2.4404e-03, -5.5203e-04,  2.9025e-03,\n",
      "        -4.4161e-03, -7.3644e-03, -8.5251e-03,  1.3178e-03,  2.1465e-03,\n",
      "        -3.7577e-03, -2.7397e-03,  4.1143e-03,  6.3600e-03, -2.5181e-03,\n",
      "         5.8973e-03,  3.5216e-03,  5.4097e-03, -7.7734e-03, -2.8055e-03,\n",
      "        -4.8055e-03, -3.5528e-03,  1.0054e-02, -5.7227e-03, -5.9349e-03,\n",
      "         8.0356e-03,  9.7974e-04, -3.4037e-03,  2.1212e-03, -4.7140e-03,\n",
      "        -1.5766e-03,  2.5833e-03,  5.6607e-03, -4.4134e-03,  4.2873e-03,\n",
      "         5.5990e-03,  4.1702e-03, -3.6006e-03, -6.6050e-03, -1.0734e-03,\n",
      "         4.1587e-04,  1.8800e-03,  9.0861e-03, -2.7616e-03,  4.2323e-03,\n",
      "         1.2901e-03,  2.5149e-03,  7.4662e-04,  5.2610e-03,  2.7076e-03,\n",
      "        -5.6982e-03,  4.1358e-04,  1.5292e-03, -5.2231e-03, -5.6114e-04,\n",
      "        -2.8083e-03,  2.7738e-03,  5.2372e-03,  1.2836e-03, -3.6090e-03,\n",
      "         7.0002e-03, -6.7250e-03,  1.2047e-03,  4.1932e-03,  4.6361e-03,\n",
      "         7.2052e-03, -8.6748e-03, -2.2045e-03,  5.3337e-03, -3.7742e-03,\n",
      "         3.7509e-03, -1.3373e-03,  4.8468e-04,  7.0749e-03,  4.0533e-03,\n",
      "        -1.4684e-03, -1.7467e-03,  6.3349e-03, -6.3700e-03,  2.4356e-03,\n",
      "         5.3682e-04, -2.1789e-03, -7.7984e-03, -3.5435e-03,  2.2231e-03,\n",
      "         1.5549e-03, -3.4156e-03,  9.5116e-03,  6.8193e-03, -3.9551e-03,\n",
      "         1.7345e-03,  1.1038e-02, -4.4787e-03, -3.3984e-03, -4.6461e-03,\n",
      "         3.4691e-03, -4.3896e-03, -5.2341e-03,  3.6531e-03,  6.5750e-03,\n",
      "         5.5547e-03, -5.2353e-03, -4.4399e-03,  7.0036e-03, -4.1564e-03,\n",
      "         5.1180e-04,  4.5499e-03, -9.1752e-03,  9.1278e-03,  2.2241e-03,\n",
      "        -3.4503e-03, -6.8971e-03,  1.5338e-02, -6.0445e-03,  2.4624e-03,\n",
      "         1.1921e-02, -6.0374e-03,  2.7444e-03,  5.7527e-03, -1.3097e-04,\n",
      "         3.9570e-03, -4.3752e-03,  7.1645e-03, -1.8757e-03,  1.2949e-03,\n",
      "         1.8569e-03, -1.1805e-03,  2.9194e-03, -5.9065e-04,  3.7765e-03,\n",
      "         5.6737e-03, -6.8377e-03,  4.4937e-04, -4.8974e-03,  1.0359e-03,\n",
      "         3.7280e-03, -4.1205e-03, -7.9062e-03,  4.5675e-03,  1.3543e-02,\n",
      "         7.3825e-04,  2.9717e-03,  3.5274e-03,  6.2243e-03,  1.6170e-03,\n",
      "        -5.1832e-03, -7.1699e-03,  3.9569e-03, -5.1746e-03,  1.1112e-02,\n",
      "        -3.7749e-03, -3.1474e-03, -2.2622e-04,  1.0319e-02, -1.5942e-04,\n",
      "         7.3028e-03,  2.9748e-03,  5.2027e-03, -8.7211e-04, -1.0130e-03,\n",
      "         7.3141e-04, -8.2265e-03, -4.5096e-03,  6.2704e-03,  7.2122e-03,\n",
      "        -7.8724e-03, -8.3062e-03, -2.2988e-03, -3.2725e-03,  1.8109e-03,\n",
      "         3.6401e-03,  6.5169e-03, -9.1461e-03,  5.5769e-04, -5.5392e-03,\n",
      "         1.7486e-03,  3.9937e-03,  6.4953e-03, -7.2768e-03, -1.1404e-02,\n",
      "         2.8498e-03, -9.1895e-05, -3.8815e-03,  3.2750e-03, -3.6753e-03,\n",
      "        -4.3099e-03,  1.2775e-02, -4.3530e-03, -7.1744e-03, -3.7723e-03,\n",
      "         3.9271e-03, -2.2709e-03,  2.1427e-03, -2.0389e-03, -3.5295e-03,\n",
      "        -1.2139e-03, -2.8593e-03,  2.3124e-03,  1.3273e-02,  2.2552e-03,\n",
      "         5.8996e-03, -1.9346e-03,  2.3624e-03,  4.2759e-03, -3.7107e-03,\n",
      "        -1.3829e-02, -8.1982e-03, -1.2866e-04,  4.4511e-03,  2.9679e-05,\n",
      "        -2.9812e-04, -1.2088e-02, -3.3779e-03,  4.2797e-03, -3.5016e-03,\n",
      "         8.5653e-03,  7.5631e-03, -8.3069e-03,  1.1426e-02,  2.7008e-03,\n",
      "         1.1263e-03,  2.0470e-03, -4.9172e-03,  4.3774e-03,  6.1225e-03,\n",
      "        -1.8042e-03,  1.6787e-04, -7.0097e-03, -1.4599e-03, -1.0280e-02,\n",
      "        -3.9889e-03,  2.2782e-03,  1.6378e-03,  6.4694e-03,  7.5487e-03,\n",
      "        -1.0359e-02, -4.8820e-03,  7.5296e-03,  5.2889e-03,  5.0081e-03,\n",
      "        -1.5664e-03,  9.0648e-03,  1.6798e-03, -1.1869e-03, -5.7785e-03,\n",
      "        -9.1497e-04,  7.2059e-03,  5.1142e-03, -1.0999e-03, -1.2630e-03,\n",
      "        -2.4263e-03,  9.8588e-03, -2.6589e-03,  3.7397e-03,  2.9911e-03,\n",
      "         5.3010e-04,  2.9471e-03, -2.5079e-03,  2.0323e-03,  1.6733e-03,\n",
      "        -5.3212e-04, -3.3120e-03,  3.4498e-03,  8.8168e-04],\n",
      "       grad_fn=<SelectBackward>) tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0627, 0.1098,\n",
      "        0.2902, 0.2549, 0.4392, 0.3686, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0706, 0.5373, 0.7137, 0.8667, 0.9647, 0.9843, 0.5686, 0.6118, 0.3333,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.6510, 0.8902, 0.9137,\n",
      "        0.4392, 0.5804, 0.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.3451, 0.7529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275,\n",
      "        0.6549, 0.5333, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1020, 0.7020, 0.8157, 0.2275,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.6471, 0.9020, 0.0941, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6235, 0.9333,\n",
      "        0.8157, 0.0510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.1412, 0.5804, 0.9686, 0.9333, 0.5294, 0.1137, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.2588, 0.6824, 0.9451, 0.9765, 0.5804, 0.1216,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.3608,\n",
      "        0.5294, 0.3804, 0.1765, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = xt_ds[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], weights[0], xt_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0684, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(preds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oD9GjH0arywA"
   },
   "source": [
    ">As you see, the ``preds`` tensor contains not only the tensor values, but also a\n",
    "gradient function. We'll use this later to do backprop.\n",
    "\n",
    ">Let's implement negative log-likelihood to use as the loss function\n",
    "(again, we can just use standard Python):\n",
    "\n",
    "Well we can't use log liklihood in this case as it is a regression problem, therefore we use mean squared error here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MM5yn2gtrywB"
   },
   "outputs": [],
   "source": [
    "def mse(input, target):\n",
    "    return torch.mean((input-target)**2)\n",
    "\n",
    "loss_func = mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4X3Up4DrywF"
   },
   "source": [
    ">Let's check our loss with our random model, so we can see if we improve\n",
    "after a backprop pass later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7-b88ZFrywG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1052, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "yb = x_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrQQ0Fl8rywK"
   },
   "source": [
    ">Let's also implement a function to calculate the accuracy of our model.\n",
    "For each prediction, if the index with the largest value matches the\n",
    "target value, then the prediction was correct.\n",
    "\n",
    "Lets not do this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtGjH1qrrywS"
   },
   "source": [
    "Let's check the accuracy of our random model, so we can see if our\n",
    "accuracy improves as our loss improves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaJfLGyjrywX"
   },
   "source": [
    "We can now run a training loop.  For each iteration, we will:\n",
    "\n",
    "- select a mini-batch of data (of size ``bs``)\n",
    "- use the model to make predictions\n",
    "- calculate the loss\n",
    "- ``loss.backward()`` updates the gradients of the model, in this case, ``weights``\n",
    "  and ``bias``.\n",
    "\n",
    "We now use these gradients to update the weights and bias.  We do this\n",
    "within the ``torch.no_grad()`` context manager, because we do not want these\n",
    "actions to be recorded for our next calculation of the gradient.  You can read\n",
    "more about how PyTorch's Autograd records operations\n",
    "`here <https://pytorch.org/docs/stable/notes/autograd.html>`_.\n",
    "\n",
    "We then set the\n",
    "gradients to zero, so that we are ready for the next loop.\n",
    "Otherwise, our gradients would record a running tally of all the operations\n",
    "that had happened (i.e. ``loss.backward()`` *adds* the gradients to whatever is\n",
    "already stored, rather than replacing them).\n",
    "\n",
    ".. tip:: You can use the standard python debugger to step through PyTorch\n",
    "   code, allowing you to check the various variable values at each step.\n",
    "   Uncomment ``set_trace()`` below to try it out.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oz8xjyeyrywX"
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.2  # learning rate\n",
    "epochs = 20  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = xt_us[start_i:end_i]\n",
    "        yb = x_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EE71t4fPrywd"
   },
   "source": [
    "That's it: we've created and trained a minimal neural network (in this case, a\n",
    "logistic regression, since we have no hidden layers) entirely from scratch!\n",
    "\n",
    "Let's check the loss and accuracy and compare those to what we got\n",
    "earlier. We expect that the loss will have decreased and accuracy to\n",
    "have increased, and they have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3eec623048>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(model(xt_ds[0]).detach().numpy().reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zmzgf9wrywe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model: tensor(0.0187, grad_fn=<MeanBackward0>)\n",
      "Loss of Binear Interpolation: tensor(0.0108)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss of the model:\", loss_func(model(xb), yb))\n",
    "print(\"Loss of Binear Interpolation:\", loss_func(xt_us, x_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLvjHRFNrywi"
   },
   "source": [
    ">Using torch.nn.functional\n",
    "------------------------------\n",
    "\n",
    ">We will now refactor our code, so that it does the same thing as before, only\n",
    "we'll start taking advantage of PyTorch's ``nn`` classes to make it more concise\n",
    "and flexible. At each step from here, we should be making our code one or more\n",
    "of: shorter, more understandable, and/or more flexible.\n",
    "\n",
    ">The first and easiest step is to make our code shorter by replacing our\n",
    "hand-written activation and loss functions with those from ``torch.nn.functional``\n",
    "(which is generally imported into the namespace ``F`` by convention). This module\n",
    "contains all the functions in the ``torch.nn`` library (whereas other parts of the\n",
    "library contain classes). As well as a wide range of loss and activation\n",
    "functions, you'll also find here some convenient functions for creating neural\n",
    "nets, such as pooling functions. (There are also functions for doing convolutions,\n",
    "linear layers, etc, but as we'll see, these are usually better handled using\n",
    "other parts of the library.)\n",
    "\n",
    ">If you're using negative log likelihood loss and log softmax activation,\n",
    "then Pytorch provides a single function ``F.cross_entropy`` that combines\n",
    "the two. So we can even remove the activation function from our model.\n",
    "\n",
    "We cannot use cross entropy\n",
    "So we use relu and mse and we put the activation function back into the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yD0kluFYrywi"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.mse_loss\n",
    "activ_func = F.relu\n",
    "\n",
    "def model(xb):\n",
    "    return activ_func(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(model(xb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S2ZpYc-rywm"
   },
   "source": [
    ">Note that we no longer call ``log_softmax`` in the ``model`` function. Let's\n",
    "confirm that our loss and accuracy are the same as before:\n",
    "\n",
    "Ok that's fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_aVx15AYrywn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0188, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZO0YyyJWrywq"
   },
   "source": [
    "Refactor using nn.Module\n",
    "-----------------------------\n",
    "Next up, we'll use ``nn.Module`` and ``nn.Parameter``, for a clearer and more\n",
    "concise training loop. We subclass ``nn.Module`` (which itself is a class and\n",
    "able to keep track of state).  In this case, we want to create a class that\n",
    "holds our weights, bias, and method for the forward step.  ``nn.Module`` has a\n",
    "number of attributes and methods (such as ``.parameters()`` and ``.zero_grad()``)\n",
    "which we will be using.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``nn.Module`` (uppercase M) is a PyTorch specific concept, and is a\n",
    "   class we'll be using a lot. ``nn.Module`` is not to be confused with the Python\n",
    "   concept of a (lowercase ``m``) `module <https://docs.python.org/3/tutorial/modules.html>`_,\n",
    "   which is a file of Python code that can be imported.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6P4-eB9arywq"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 784) / 784)\n",
    "        self.bias = nn.Parameter(torch.zeros(784))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return activ_func(xb @ self.weights + self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XTWSNwtryww"
   },
   "source": [
    "Since we're now using an object instead of just using a function, we\n",
    "first have to instantiate our model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXEIkDHqrywy"
   },
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pStDDMckryw0"
   },
   "source": [
    "Now we can calculate the loss in the same way as before. Note that\n",
    "``nn.Module`` objects are used as if they are functions (i.e they are\n",
    "*callable*), but behind the scenes Pytorch will call our ``forward``\n",
    "method automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzlCRR4Oryw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1144, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ncGx1KOryw4"
   },
   "source": [
    "Previously for our training loop we had to update the values for each parameter\n",
    "by name, and manually zero out the grads for each parameter separately, like this:\n",
    "::\n",
    "  with torch.no_grad():\n",
    "      weights -= weights.grad * lr\n",
    "      bias -= bias.grad * lr\n",
    "      weights.grad.zero_()\n",
    "      bias.grad.zero_()\n",
    "\n",
    "\n",
    "Now we can take advantage of model.parameters() and model.zero_grad() (which\n",
    "are both defined by PyTorch for ``nn.Module``) to make those steps more concise\n",
    "and less prone to the error of forgetting some of our parameters, particularly\n",
    "if we had a more complicated model:\n",
    "::\n",
    "  with torch.no_grad():\n",
    "      for p in model.parameters(): p -= p.grad * lr\n",
    "      model.zero_grad()\n",
    "\n",
    "\n",
    "We'll wrap our little training loop in a ``fit`` function so we can run it\n",
    "again later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(xb)\n",
    "xb.size()\n",
    "bs\n",
    "x_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wp2a3kU-ryw5"
   },
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = xt_us[start_i:end_i]\n",
    "            yb = x_train[start_i:end_i]\n",
    "            xb.size()\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMucwk_Kryw9"
   },
   "source": [
    "Let's double-check that our loss has gone down:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGvSWdEpryxA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0082, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0071407687, 0.9117204455330581]\n",
      "torch.Size([100, 784])\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/qg35/jlgao2/venvs/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "x_sample = model(xt_ds[0:100])\n",
    "y_sample = x_train[0:100]\n",
    "\n",
    "\n",
    "\n",
    "def testTensors(x, y):\n",
    "    mse = []\n",
    "    ssim = []\n",
    "    x = x.detach().numpy()\n",
    "    y = y.detach().numpy()\n",
    "    for i in range(100):\n",
    "        mse.append(np.average(np.square(y[i]-x[i])))\n",
    "        ssim.append(compare_ssim(y[i],x[i]))\n",
    "    return [np.mean(mse), np.mean(ssim)]\n",
    "\n",
    "print(testTensors(x_sample, y_sample))\n",
    "        \n",
    "#     e5_mse =  \n",
    "# from skimage.measure import compare_ssim as ssim\n",
    "# print(\"The MSE of bicubic interpolation on this image is\", e5_mse)\n",
    "# e5_ssim = \n",
    "# print(\"The SSIM of bicubic interpolation on this image is\", e5_ssim)\n",
    "print(x_sample.shape)\n",
    "\n",
    "print(np.mean(mse))\n",
    "print(np.mean(ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3eec586d68>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(x_sample[0].detach().numpy().reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9fOqVvNryxE"
   },
   "source": [
    "Refactor using nn.Linear\n",
    "-------------------------\n",
    "\n",
    "We continue to refactor our code.  Instead of manually defining and\n",
    "initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @\n",
    "self.weights + self.bias``, we will instead use the Pytorch class\n",
    "`nn.Linear <https://pytorch.org/docs/stable/nn.html#linear-layers>`_ for a\n",
    "linear layer, which does all that for us. Pytorch has many types of\n",
    "predefined layers that can greatly simplify our code, and often makes it\n",
    "faster too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TH0xxYm2ryxF"
   },
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 784)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return activ_func(self.lin(xb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIUqZuKKryxJ"
   },
   "source": [
    "We instantiate our model and calculate the loss in the same way as before:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fShvHQIryxJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1094, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZu5_JudryxM"
   },
   "source": [
    "We are still able to use our same ``fit`` method as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0YPzJfLryxR"
   },
   "source": [
    "Refactor using optim\n",
    "------------------------------\n",
    "\n",
    "Pytorch also has a package with various optimization algorithms, ``torch.optim``.\n",
    "We can use the ``step`` method from our optimizer to take a forward step, instead\n",
    "of manually updating each parameter.\n",
    "\n",
    "This will let us replace our previous manually coded optimization step:\n",
    "::\n",
    "  with torch.no_grad():\n",
    "      for p in model.parameters(): p -= p.grad * lr\n",
    "      model.zero_grad()\n",
    "\n",
    "and instead use just:\n",
    "::\n",
    "  opt.step()\n",
    "  opt.zero_grad()\n",
    "\n",
    "(``optim.zero_grad()`` resets the gradient to 0 and we need to call it before\n",
    "computing the gradient for the next minibatch.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYpxP7WtryxS"
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXpp6IvaryxV"
   },
   "source": [
    "We'll define a little function to create our model and optimizer so we\n",
    "can reuse it in the future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1wkfRciZryxW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1145, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0119, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = xt_us[start_i:end_i]\n",
    "        yb = x_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt_us.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSonohrqryxZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XeoIYKWryxb"
   },
   "source": [
    "Both ``x_train`` and ``y_train`` can be combined in a single ``TensorDataset``,\n",
    "which will be easier to iterate over and slice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qkotC3cryx5"
   },
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(xt_us, x_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(xv_us, x_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCRYjVQHryx6"
   },
   "source": [
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call ``model.train()`` before training, and ``model.eval()``\n",
    "before inference, because these are used by layers such as ``nn.BatchNorm2d``\n",
    "and ``nn.Dropout`` to ensure appropriate behaviour for these different phases.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "baVc72zeryx8"
   },
   "source": [
    "Create fit() and get_data()\n",
    "----------------------------------\n",
    "\n",
    "We'll now do a little refactoring of our own. Since we go through a similar\n",
    "process twice of calculating the loss for both the training set and the\n",
    "validation set, let's make that into its own function, ``loss_batch``, which\n",
    "computes the loss for one batch.\n",
    "\n",
    "We pass an optimizer in for the training set, and use it to perform\n",
    "backprop.  For the validation set, we don't pass an optimizer, so the\n",
    "method doesn't perform backprop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnC4QnJvryx9"
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUelO-cOryx_"
   },
   "source": [
    "``fit`` runs the necessary operations to train our model and compute the\n",
    "training and validation losses for each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmz91ViuryyA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gr6yTe8pryyC"
   },
   "source": [
    "``get_data`` returns dataloaders for the training and validation sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3b9j3HmkryyC"
   },
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdeKaQQdryyE"
   },
   "source": [
    "Now, our whole process of obtaining the data loaders and fitting the\n",
    "model can be run in 3 lines of code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHugCSAUryyF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.043536354666948315\n",
      "1 0.034637879544496535\n",
      "2 0.02981453269124031\n",
      "3 0.026702913030982017\n",
      "4 0.024470594796538354\n",
      "5 0.02276384028196335\n",
      "6 0.02131422400176525\n",
      "7 0.02016535888314247\n",
      "8 0.01920258931517601\n",
      "9 0.018377615958452224\n",
      "10 0.017659854301810263\n",
      "11 0.017027694776654244\n",
      "12 0.016467397597432137\n",
      "13 0.015964249891042708\n",
      "14 0.015511864733695984\n",
      "15 0.015098925122618675\n",
      "16 0.014722148871421814\n",
      "17 0.014376439225673675\n",
      "18 0.014057751497626305\n",
      "19 0.013763205996155738\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8v_hJ62tryyH"
   },
   "source": [
    "You can use these basic 3 lines of code to train a wide variety of models.\n",
    "Let's see if we can use them to train a convolutional neural network (CNN)!\n",
    "\n",
    "Switch to CNN\n",
    "-------------\n",
    "\n",
    "We are now going to build our neural network with three convolutional layers.\n",
    "Because none of the functions in the previous section assume anything about\n",
    "the model form, we'll be able to use them to train a CNN without any modification.\n",
    "\n",
    "We will use Pytorch's predefined\n",
    "`Conv2d <https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`_ class\n",
    "as our convolutional layer. We define a CNN with 3 convolutional layers.\n",
    "Each convolution is followed by a ReLU.  At the end, we perform an\n",
    "average pooling.  (Note that ``view`` is PyTorch's version of numpy's\n",
    "``reshape``)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KRuHK7qPryyK"
   },
   "source": [
    "`Momentum <https://cs231n.github.io/neural-networks-3/#sgd>`_ is a variation on\n",
    "stochastic gradient descent that takes previous updates into account as well\n",
    "and generally leads to faster training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(xb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 784])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hboRZNYRryyN"
   },
   "source": [
    "nn.Sequential\n",
    "------------------------\n",
    "\n",
    "``torch.nn`` has another handy class we can use to simply our code:\n",
    "`Sequential <https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential>`_ .\n",
    "A ``Sequential`` object runs each of the modules contained within it, in a\n",
    "sequential manner. This is a simpler way of writing our neural network.\n",
    "\n",
    "To take advantage of this, we need to be able to easily define a\n",
    "**custom layer** from a given function.  For instance, PyTorch doesn't\n",
    "have a `view` layer, and we need to create one for our network. ``Lambda``\n",
    "will create a layer that we can then use when defining a network with\n",
    "``Sequential``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uih149xZryyN"
   },
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-nUNbKEryyO"
   },
   "source": [
    "The model created with ``Sequential`` is simply:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVEsJYt2ryyR"
   },
   "source": [
    "Wrapping DataLoader\n",
    "-----------------------------\n",
    "\n",
    "Our CNN is fairly concise, but it only works with MNIST, because:\n",
    " - It assumes the input is a 28\\*28 long vector\n",
    " - It assumes that the final CNN grid size is 4\\*4 (since that's the average\n",
    "pooling kernel size we used)\n",
    "\n",
    "Let's get rid of these two assumptions, so our model works with any 2d\n",
    "single channel image. First, we can remove the initial Lambda layer but\n",
    "moving the data preprocessing into a generator:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2aIhV0dZryyS"
   },
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdnBhbx5ryyU"
   },
   "source": [
    "Next, we can replace ``nn.AvgPool2d`` with ``nn.AdaptiveAvgPool2d``, which\n",
    "allows us to define the size of the *output* tensor we want, rather than\n",
    "the *input* tensor we have. As a result, our model will work with any\n",
    "size input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfrPtblzryyV"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            #28*28*1\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            #14*14*16\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            #14*14*32\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            #28x28x16\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=1, padding=1, bias=False)\n",
    "            #28x28x1   \n",
    "        )\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlkeeNkUryya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JUToGMgryyd"
   },
   "source": [
    "And then create a device object for it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vTHPc1rryyd"
   },
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMLds7v2ryyg"
   },
   "source": [
    "Let's update ``preprocess`` to move batches to the GPU:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UsSTodAMryyg"
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzTL65dSryyi"
   },
   "source": [
    "Finally, we can move our model to the GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikzj91yJryyi"
   },
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38r55mXMryyk"
   },
   "source": [
    "You should find it runs faster now:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSgSG2czryyk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.006070752645283938\n",
      "1 0.006013445632904768\n",
      "2 0.005957098051160574\n",
      "3 0.005905575553327799\n",
      "4 0.005858712063729763\n",
      "5 0.005814067430049181\n",
      "6 0.005769696255773306\n",
      "7 0.0057284526631236074\n",
      "8 0.005689675119519234\n",
      "9 0.0056561275467276574\n",
      "10 0.00561918348968029\n",
      "11 0.00558917403370142\n",
      "12 0.005557484621554613\n",
      "13 0.005526570887118578\n",
      "14 0.005500036523491144\n",
      "15 0.0054724472790956494\n",
      "16 0.005446424344927072\n",
      "17 0.005421431645005941\n",
      "18 0.005398137820512057\n",
      "19 0.00537549922093749\n",
      "20 0.0053534118108451365\n",
      "21 0.0053330804571509365\n",
      "22 0.005311884227395058\n",
      "23 0.00529219608977437\n",
      "24 0.0052737728275358674\n",
      "25 0.005255887055397034\n",
      "26 0.005237679877877236\n",
      "27 0.0052215172566473485\n",
      "28 0.005204095690697431\n",
      "29 0.0051886571899056435\n"
     ]
    }
   ],
   "source": [
    "fit(30, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0051259575, 0.9390531106113823]\n",
      "torch.Size([100, 784])\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/qg35/jlgao2/venvs/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "x_sample = model(xt_us[0:100].to(dev)).to('cpu')\n",
    "y_sample = x_train[0:100]\n",
    "\n",
    "def testTensors(x, y):\n",
    "    mse = []\n",
    "    ssim = []\n",
    "    x = x.detach().numpy()\n",
    "    y = y.detach().numpy()\n",
    "    for i in range(100):\n",
    "        mse.append(np.average(np.square(y[i]-x[i])))\n",
    "        ssim.append(compare_ssim(y[i],x[i]))\n",
    "    return [np.mean(mse), np.mean(ssim)]\n",
    "\n",
    "print(testTensors(x_sample, y_sample))\n",
    "        \n",
    "#     e5_mse =  \n",
    "# from skimage.measure import compare_ssim as ssim\n",
    "# print(\"The MSE of bicubic interpolation on this image is\", e5_mse)\n",
    "# e5_ssim = \n",
    "# print(\"The SSIM of bicubic interpolation on this image is\", e5_ssim)\n",
    "print(x_sample.shape)\n",
    "\n",
    "print(np.mean(mse))\n",
    "print(np.mean(ssim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_cnn = pyplot.imshow(x_sample[0].detach().numpy().reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_3x06iDryyn"
   },
   "source": [
    "Closing thoughts\n",
    "-----------------\n",
    "\n",
    "We now have a general data pipeline and training loop which you can use for\n",
    "training many types of models using Pytorch. To see how simple training a model\n",
    "can now be, take a look at the `mnist_sample` sample notebook.\n",
    "\n",
    "Of course, there are many things you'll want to add, such as data augmentation,\n",
    "hyperparameter tuning, monitoring training, transfer learning, and so forth.\n",
    "These features are available in the fastai library, which has been developed\n",
    "using the same design approach shown in this tutorial, providing a natural\n",
    "next step for practitioners looking to take their models further.\n",
    "\n",
    "We promised at the start of this tutorial we'd explain through example each of\n",
    "``torch.nn``, ``torch.optim``, ``Dataset``, and ``DataLoader``. So let's summarize\n",
    "what we've seen:\n",
    "\n",
    " - **torch.nn**\n",
    "\n",
    "   + ``Module``: creates a callable which behaves like a function, but can also\n",
    "     contain state(such as neural net layer weights). It knows what ``Parameter`` (s) it\n",
    "     contains and can zero all their gradients, loop through them for weight updates, etc.\n",
    "   + ``Parameter``: a wrapper for a tensor that tells a ``Module`` that it has weights\n",
    "     that need updating during backprop. Only tensors with the `requires_grad` attribute set are updated\n",
    "   + ``functional``: a module(usually imported into the ``F`` namespace by convention)\n",
    "     which contains activation functions, loss functions, etc, as well as non-stateful\n",
    "     versions of layers such as convolutional and linear layers.\n",
    " - ``torch.optim``: Contains optimizers such as ``SGD``, which update the weights\n",
    "   of ``Parameter`` during the backward step\n",
    " - ``Dataset``: An abstract interface of objects with a ``__len__`` and a ``__getitem__``,\n",
    "   including classes provided with Pytorch such as ``TensorDataset``\n",
    " - ``DataLoader``: Takes any ``Dataset`` and creates an iterator which returns batches of data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [
    "sLvjHRFNrywi",
    "ZO0YyyJWrywq",
    "e9fOqVvNryxE",
    "M0YPzJfLryxR",
    "BCtYLuLNryxZ",
    "jXDC6amFryxm",
    "zRFhBk_Cryx4",
    "baVc72zeryx8",
    "8v_hJ62tryyH",
    "hboRZNYRryyN",
    "TVEsJYt2ryyR",
    "1p_XKbRyryya",
    "M_3x06iDryyn"
   ],
   "name": "MNIST Up and Down Sampling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
