{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as tfms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "data_dir = '/data/h5'\n",
    "\n",
    "root_dir = os.getcwd()+data_dir\n",
    "\n",
    "# Number of workers for dataloader\n",
    "# Issues with python for windows when workers>0 WE ON LINUX NOW BBY\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training P100 TIME\n",
    "batch_size = 4\n",
    "\n",
    "# Spatial size of training images. \n",
    "size = 400\n",
    "\n",
    "#image scaling factor\n",
    "scale = 2\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.00005\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "img_ext = '.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "lr_transforms = transforms.Compose([transforms.Resize(size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "hr_transforms = transforms.Compose([transforms.Resize(size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatSRDataset(Dataset):\n",
    "    def __init__(self, root_dir, img_ext, scale_factor, lr_transform=None, hr_transform=None):\n",
    "        \"\"\"\n",
    "        :param data_folder: # folder with JSON data files\n",
    "        :param split: one of 'train' or 'test'\n",
    "        :param crop_size: crop size of target HR images\n",
    "        :param scaling_factor: the input LR images will be downsampled from the target HR images by this factor; the scaling done in the super-resolution\n",
    "        :param img_type: the format for images supplied to the model; see convert_image() in utils.py for available formats\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.img_ext = img_ext\n",
    "        self.scale_factor = scale_factor\n",
    "        self.hr_images = []\n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "        \n",
    "                \n",
    "        #Get LR file names\n",
    "        for dirName, _, fileList in os.walk(self.root_dir+'/hr_images'):\n",
    "            for filename in fileList:\n",
    "                if filename.endswith(self.img_ext):\n",
    "                    self.hr_images.append(dirName + '/' + filename)\n",
    "                    \n",
    "        self.sz = PIL.Image.open(self.hr_images[0]).size[0]\n",
    "        self.lr_resize = tfms.Resize(int(self.sz/self.scale_factor))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #return image pair\n",
    "        self.hr_img = PIL.Image.open(self.hr_images[idx])\n",
    "        self.lr_img = self.lr_resize(self.hr_img)\n",
    "        \n",
    "        self.sample = [self.lr_transform(self.lr_image), self.hr_transform(self.hr_image)]\n",
    "                                \n",
    "\n",
    "        return self.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise dataset\n",
    "satdataset = SatSRDataset(\n",
    "                     root_dir, img_ext=img_ext, scale_factor=scale,lr_transform=lr_transforms,hr_transform=hr_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise dataloader \n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    satdataset, \n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=workers, pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(, , 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
