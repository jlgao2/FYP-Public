{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import torchvision.transforms.functional as FT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Some constants\n",
    "rgb_weights = torch.FloatTensor([65.481, 128.553, 24.966]).to(device)\n",
    "imagenet_mean = torch.FloatTensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n",
    "imagenet_std = torch.FloatTensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)\n",
    "imagenet_mean_cuda = torch.FloatTensor([0.485, 0.456, 0.406]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "imagenet_std_cuda = torch.FloatTensor([0.229, 0.224, 0.225]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "\n",
    "def create_data_lists(train_folders, test_folders, min_size, output_folder):\n",
    "    \"\"\"\n",
    "    Create lists for images in the training set and each of the test sets.\n",
    "    :param train_folders: folders containing the training images; these will be merged\n",
    "    :param test_folders: folders containing the test images; each test folder will form its own test set\n",
    "    :param min_size: minimum width and height of images to be considered\n",
    "    :param output_folder: save data lists here\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating data lists... this may take some time.\\n\")\n",
    "    train_images = list()\n",
    "    for d in train_folders:\n",
    "        for i in os.listdir(d):\n",
    "            img_path = os.path.join(d, i)\n",
    "            img = Image.open(img_path, mode='r')\n",
    "            if img.width >= min_size and img.height >= min_size:\n",
    "                train_images.append(img_path)\n",
    "    print(\"There are %d images in the training data.\\n\" % len(train_images))\n",
    "    with open(os.path.join(output_folder, 'train_images.json'), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "\n",
    "    for d in test_folders:\n",
    "        test_images = list()\n",
    "        test_name = d.split(\"/\")[-1]\n",
    "        for i in os.listdir(d):\n",
    "            img_path = os.path.join(d, i)\n",
    "            img = Image.open(img_path, mode='r')\n",
    "            if img.width >= min_size and img.height >= min_size:\n",
    "                test_images.append(img_path)\n",
    "        print(\"There are %d images in the %s test data.\\n\" % (len(test_images), test_name))\n",
    "        with open(os.path.join(output_folder, test_name + '_test_images.json'), 'w') as j:\n",
    "            json.dump(test_images, j)\n",
    "\n",
    "    print(\"JSONS containing lists of Train and Test images have been saved to %s\\n\" % output_folder)\n",
    "\n",
    "\n",
    "def convert_image(img, source, target):\n",
    "    \"\"\"\n",
    "    Convert an image from a source format to a target format.\n",
    "    :param img: image\n",
    "    :param source: source format, one of 'pil' (PIL image), '[0, 1]' or '[-1, 1]' (pixel value ranges)\n",
    "    :param target: target format, one of 'pil' (PIL image), '[0, 255]', '[0, 1]', '[-1, 1]' (pixel value ranges),\n",
    "                   'imagenet-norm' (pixel values standardized by imagenet mean and std.),\n",
    "                   'y-channel' (luminance channel Y in the YCbCr color format, used to calculate PSNR and SSIM)\n",
    "    :return: converted image\n",
    "    \"\"\"\n",
    "    assert source in {'pil', '[0, 1]', '[-1, 1]'}, \"Cannot convert from source format %s!\" % source\n",
    "    assert target in {'pil', '[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm',\n",
    "                      'y-channel'}, \"Cannot convert to target format %s!\" % target\n",
    "\n",
    "    # Convert from source to [0, 1]\n",
    "    if source == 'pil':\n",
    "        img = FT.to_tensor(img)\n",
    "\n",
    "    elif source == '[0, 1]':\n",
    "        pass  # already in [0, 1]\n",
    "\n",
    "    elif source == '[-1, 1]':\n",
    "        img = (img + 1.) / 2.\n",
    "\n",
    "    # Convert from [0, 1] to target\n",
    "    if target == 'pil':\n",
    "        img = FT.to_pil_image(img)\n",
    "\n",
    "    elif target == '[0, 255]':\n",
    "        img = 255. * img\n",
    "\n",
    "    elif target == '[0, 1]':\n",
    "        pass  # already in [0, 1]\n",
    "\n",
    "    elif target == '[-1, 1]':\n",
    "        img = 2. * img - 1.\n",
    "\n",
    "    elif target == 'imagenet-norm':\n",
    "        if img.ndimension() == 3:\n",
    "            img = (img - imagenet_mean) / imagenet_std\n",
    "        elif img.ndimension() == 4:\n",
    "            img = (img - imagenet_mean_cuda) / imagenet_std_cuda\n",
    "\n",
    "    elif target == 'y-channel':\n",
    "        # Based on definitions at https://github.com/xinntao/BasicSR/wiki/Color-conversion-in-SR\n",
    "        # torch.dot() does not work the same way as numpy.dot()\n",
    "        # So, use torch.matmul() to find the dot product between the last dimension of an 4-D tensor and a 1-D tensor\n",
    "        img = torch.matmul(255. * img.permute(0, 2, 3, 1)[:, 4:-4, 4:-4, :], rgb_weights) / 255. + 16.\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "class ImageTransforms(object):\n",
    "    \"\"\"\n",
    "    Image transformation pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, crop_size, scaling_factor, lr_img_type, hr_img_type):\n",
    "        \"\"\"\n",
    "        :param split: one of 'train' or 'test'\n",
    "        :param crop_size: crop size of HR images\n",
    "        :param scaling_factor: LR images will be downsampled from the HR images by this factor\n",
    "        :param lr_img_type: the target format for the LR image; see convert_image() above for available formats\n",
    "        :param hr_img_type: the target format for the HR image; see convert_image() above for available formats\n",
    "        \"\"\"\n",
    "        self.split = split.lower()\n",
    "        self.crop_size = crop_size\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.lr_img_type = lr_img_type\n",
    "        self.hr_img_type = hr_img_type\n",
    "\n",
    "        assert self.split in {'train', 'test'}\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        :param img: a PIL source image from which the HR image will be cropped, and then downsampled to create the LR image\n",
    "        :return: LR and HR images in the specified format\n",
    "        \"\"\"\n",
    "\n",
    "        # Crop\n",
    "        if self.split == 'train':\n",
    "            # Take a random fixed-size crop of the image, which will serve as the high-resolution (HR) image\n",
    "            left = random.randint(1, img.width - self.crop_size)\n",
    "            top = random.randint(1, img.height - self.crop_size)\n",
    "            right = left + self.crop_size\n",
    "            bottom = top + self.crop_size\n",
    "            hr_img = img.crop((left, top, right, bottom))\n",
    "        else:\n",
    "            # Take the largest possible center-crop of it such that its dimensions are perfectly divisible by the scaling factor\n",
    "            x_remainder = img.width % self.scaling_factor\n",
    "            y_remainder = img.height % self.scaling_factor\n",
    "            left = x_remainder // 2\n",
    "            top = y_remainder // 2\n",
    "            right = left + (img.width - x_remainder)\n",
    "            bottom = top + (img.height - y_remainder)\n",
    "            hr_img = img.crop((left, top, right, bottom))\n",
    "\n",
    "        # Downsize this crop to obtain a low-resolution version of it\n",
    "        lr_img = hr_img.resize((int(hr_img.width / self.scaling_factor), int(hr_img.height / self.scaling_factor)),\n",
    "                               Image.BICUBIC)\n",
    "\n",
    "        # Sanity check\n",
    "        assert hr_img.width == lr_img.width * self.scaling_factor and hr_img.height == lr_img.height * self.scaling_factor\n",
    "\n",
    "        # Convert the LR and HR image to the required type\n",
    "        lr_img = convert_image(lr_img, source='pil', target=self.lr_img_type)\n",
    "        hr_img = convert_image(hr_img, source='pil', target=self.hr_img_type)\n",
    "\n",
    "        return lr_img, hr_img\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    \"\"\"\n",
    "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
    "    :param optimizer: optimizer with the gradients to be clipped\n",
    "    :param grad_clip: clip value\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param state: checkpoint contents\n",
    "    \"\"\"\n",
    "\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "    \"\"\"\n",
    "    Shrinks learning rate by a specified factor.\n",
    "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
    "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nDECAYING learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to be used by a PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, split, crop_size, scaling_factor, lr_img_type, hr_img_type, test_data_name=None):\n",
    "        \"\"\"\n",
    "        :param data_folder: # folder with JSON data files\n",
    "        :param split: one of 'train' or 'test'\n",
    "        :param crop_size: crop size of target HR images\n",
    "        :param scaling_factor: the input LR images will be downsampled from the target HR images by this factor; the scaling done in the super-resolution\n",
    "        :param lr_img_type: the format for the LR image supplied to the model; see convert_image() in utils.py for available formats\n",
    "        :param hr_img_type: the format for the HR image supplied to the model; see convert_image() in utils.py for available formats\n",
    "        :param test_data_name: if this is the 'test' split, which test dataset? (for example, \"Set14\")\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.split = split.lower()\n",
    "        self.crop_size = int(crop_size)\n",
    "        self.scaling_factor = int(scaling_factor)\n",
    "        self.lr_img_type = lr_img_type\n",
    "        self.hr_img_type = hr_img_type\n",
    "        self.test_data_name = test_data_name\n",
    "\n",
    "        assert self.split in {'train', 'test'}\n",
    "        if self.split == 'test' and self.test_data_name is None:\n",
    "            raise ValueError(\"Please provide the name of the test dataset!\")\n",
    "        assert lr_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm'}\n",
    "        assert hr_img_type in {'[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm'}\n",
    "\n",
    "        # If this is a training dataset, then crop dimensions must be perfectly divisible by the scaling factor\n",
    "        # (If this is a test dataset, images are not cropped to a fixed size, so this variable isn't used)\n",
    "        if self.split == 'train':\n",
    "            assert self.crop_size % self.scaling_factor == 0, \"Crop dimensions are not perfectly divisible by scaling factor! This will lead to a mismatch in the dimensions of the original HR patches and their super-resolved (SR) versions!\"\n",
    "\n",
    "        # Read list of image-paths\n",
    "        if self.split == 'train':\n",
    "            with open(os.path.join(data_folder, 'train_images.json'), 'r') as j:\n",
    "                self.images = json.load(j)\n",
    "        else:\n",
    "            with open(os.path.join(data_folder, self.test_data_name + '_test_images.json'), 'r') as j:\n",
    "                self.images = json.load(j)\n",
    "\n",
    "        # Select the correct set of transforms\n",
    "        self.transform = ImageTransforms(split=self.split,\n",
    "                                         crop_size=self.crop_size,\n",
    "                                         scaling_factor=self.scaling_factor,\n",
    "                                         lr_img_type=self.lr_img_type,\n",
    "                                         hr_img_type=self.hr_img_type)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        This method is required to be defined for use in the PyTorch DataLoader.\n",
    "        :param i: index to retrieve\n",
    "        :return: the 'i'th pair LR and HR images to be fed into the model\n",
    "        \"\"\"\n",
    "        # Read image\n",
    "        img = Image.open(self.images[i], mode='r')\n",
    "        img = img.convert('RGB')\n",
    "        if img.width <= 96 or img.height <= 96:\n",
    "            print(self.images[i], img.width, img.height)\n",
    "        lr_img, hr_img = self.transform(img)\n",
    "\n",
    "        return lr_img, hr_img\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method is required to be defined for use in the PyTorch DataLoader.\n",
    "        :return: size of this data (in number of images)\n",
    "        \"\"\"\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional block, comprising convolutional, BN, activation layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n",
    "        \"\"\"\n",
    "        :param in_channels: number of input channels\n",
    "        :param out_channels: number of output channe;s\n",
    "        :param kernel_size: kernel size\n",
    "        :param stride: stride\n",
    "        :param batch_norm: include a BN layer?\n",
    "        :param activation: Type of activation; None if none\n",
    "        \"\"\"\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "\n",
    "        if activation is not None:\n",
    "            activation = activation.lower()\n",
    "            assert activation in {'prelu', 'leakyrelu', 'tanh'}\n",
    "\n",
    "        # A container that will hold the layers in this convolutional block\n",
    "        layers = list()\n",
    "\n",
    "        # A convolutional layer\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=kernel_size // 2))\n",
    "\n",
    "        # A batch normalization (BN) layer, if wanted\n",
    "        if batch_norm is True:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "\n",
    "        # An activation layer, if wanted\n",
    "        if activation == 'prelu':\n",
    "            layers.append(nn.PReLU())\n",
    "        elif activation == 'leakyrelu':\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        elif activation == 'tanh':\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # Put together the convolutional block as a sequence of the layers in this container\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param input: input images, a tensor of size (N, in_channels, w, h)\n",
    "        :return: output images, a tensor of size (N, out_channels, w, h)\n",
    "        \"\"\"\n",
    "        output = self.conv_block(input)  # (N, out_channels, w, h)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubPixelConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A subpixel convolutional block, comprising convolutional, pixel-shuffle, and PReLU activation layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=3, n_channels=64, scaling_factor=2):\n",
    "        \"\"\"\n",
    "        :param kernel_size: kernel size of the convolution\n",
    "        :param n_channels: number of input and output channels\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions)\n",
    "        \"\"\"\n",
    "        super(SubPixelConvolutionalBlock, self).__init__()\n",
    "\n",
    "        # A convolutional layer that increases the number of channels by scaling factor^2, followed by pixel shuffle and PReLU\n",
    "        self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels * (scaling_factor ** 2),\n",
    "                              kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        # These additional channels are shuffled to form additional pixels, upscaling each dimension by the scaling factor\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
    "        :return: scaled output images, a tensor of size (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        output = self.conv(input)  # (N, n_channels * scaling factor^2, w, h)\n",
    "        output = self.pixel_shuffle(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        output = self.prelu(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block, comprising two convolutional blocks with a residual connection across them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=3, n_channels=64):\n",
    "        \"\"\"\n",
    "        :param kernel_size: kernel size\n",
    "        :param n_channels: number of input and output channels (same because the input must be added to the output)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # The first convolutional block\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation='PReLu')\n",
    "\n",
    "        # The second convolutional block\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
    "        :return: output images, a tensor of size (N, n_channels, w, h)\n",
    "        \"\"\"\n",
    "        residual = input  # (N, n_channels, w, h)\n",
    "        output = self.conv_block1(input)  # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
    "        output = output + residual  # (N, n_channels, w, h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The SRResNet, as defined in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n",
    "        \"\"\"\n",
    "        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "        :param n_blocks: number of residual blocks\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n",
    "        \"\"\"\n",
    "        super(SRResNet, self).__init__()\n",
    "\n",
    "        # Scaling factor must be 2, 4, or 8\n",
    "        scaling_factor = int(scaling_factor)\n",
    "        assert scaling_factor in {2, 4, 8}, \"The scaling factor must be 2, 4, or 8!\"\n",
    "\n",
    "        # The first convolutional block\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='PReLu')\n",
    "\n",
    "        # A sequence of n_blocks residual blocks, each containing a skip-connection across the block\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(kernel_size=small_kernel_size, n_channels=n_channels) for i in range(n_blocks)])\n",
    "\n",
    "        # Another convolutional block\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels,\n",
    "                                              kernel_size=small_kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "        # Upscaling is done by sub-pixel convolution, with each such block upscaling by a factor of 2\n",
    "        n_subpixel_convolution_blocks = int(math.log2(scaling_factor))\n",
    "        self.subpixel_convolutional_blocks = nn.Sequential(\n",
    "            *[SubPixelConvolutionalBlock(kernel_size=small_kernel_size, n_channels=n_channels, scaling_factor=2) for i\n",
    "              in range(n_subpixel_convolution_blocks)])\n",
    "\n",
    "        # The last convolutional block\n",
    "        self.conv_block3 = ConvolutionalBlock(in_channels=n_channels, out_channels=3, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='Tanh')\n",
    "\n",
    "    def forward(self, lr_imgs):\n",
    "        \"\"\"\n",
    "        Forward prop.\n",
    "        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n",
    "        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        output = self.conv_block1(lr_imgs)  # (N, 3, w, h)\n",
    "        residual = output  # (N, n_channels, w, h)\n",
    "        output = self.residual_blocks(output)  # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
    "        output = output + residual  # (N, n_channels, w, h)\n",
    "        output = self.subpixel_convolutional_blocks(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        sr_imgs = self.conv_block3(output)  # (N, 3, w * scaling factor, h * scaling factor)\n",
    "\n",
    "        return sr_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so what we pretty much have to do is to use this network on an image, output should have same channels and more resolution because this code is for semantic segmentation. Also need to be careful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "data_folder = './'  # folder with JSON data files\n",
    "crop_size = 96  # crop size of target HR images\n",
    "scaling_factor = 4  # the scaling factor for the generator; the input LR images will be downsampled from the target HR images by this factor\n",
    "\n",
    "# Model parameters\n",
    "large_kernel_size = 9  # kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "small_kernel_size = 3  # kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "n_channels = 64  # number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "n_blocks = 16  # number of residual blocks\n",
    "\n",
    "# Learning parameters\n",
    "checkpoint = None  # path to model checkpoint, None if none\n",
    "batch_size = 16  # batch size\n",
    "start_epoch = 0  # start at this epoch\n",
    "iterations = 1e6  # number of training iterations\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "print_freq = 500  # print training status once every __ batches\n",
    "lr = 1e-4  # learning rate\n",
    "grad_clip = None  # clip if gradients are exploding\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training.\n",
    "    \"\"\"\n",
    "    global start_epoch, epoch, checkpoint\n",
    "\n",
    "    # Initialize model or load checkpoint\n",
    "    if checkpoint is None:\n",
    "        model = SRResNet(large_kernel_size=large_kernel_size, small_kernel_size=small_kernel_size,\n",
    "                         n_channels=n_channels, n_blocks=n_blocks, scaling_factor=scaling_factor)\n",
    "        # Initialize the optimizer\n",
    "        optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                     lr=lr)\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        model = checkpoint['model']\n",
    "        optimizer = checkpoint['optimizer']\n",
    "\n",
    "    # Move to default device\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    # Custom dataloaders\n",
    "    train_dataset = SRDataset(data_folder,\n",
    "                              split='train',\n",
    "                              crop_size=crop_size,\n",
    "                              scaling_factor=scaling_factor,\n",
    "                              lr_img_type='imagenet-norm',\n",
    "                              hr_img_type='[-1, 1]')\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers,\n",
    "                                               pin_memory=True)  # note that we're passing the collate function here\n",
    "\n",
    "    # Total number of epochs to train for\n",
    "    epochs = int(iterations // len(train_loader) + 1)\n",
    "\n",
    "    # Epochs\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # One epoch's training\n",
    "        train(train_loader=train_loader,\n",
    "              model=model,\n",
    "              criterion=criterion,\n",
    "              optimizer=optimizer,\n",
    "              epoch=epoch)\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model': model,\n",
    "                    'optimizer': optimizer},\n",
    "                   'checkpoint_srresnet.pth.tar')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: content loss function (Mean Squared-Error loss)\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables batch normalization\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        lr_imgs = lr_imgs.to(device)  # (batch_size (N), 3, 24, 24), imagenet-normed\n",
    "        hr_imgs = hr_imgs.to(device)  # (batch_size (N), 3, 96, 96), in [-1, 1]\n",
    "\n",
    "        # Forward prop.\n",
    "        sr_imgs = model(lr_imgs)  # (N, 3, 96, 96), in [-1, 1]\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(sr_imgs, hr_imgs)  # scalar\n",
    "\n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep track of loss\n",
    "        losses.update(loss.item(), lr_imgs.size(0))\n",
    "\n",
    "        # Keep track of batch time\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        # Reset start time\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]----'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})----'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})----'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(epoch, i, len(train_loader),\n",
    "                                                                    batch_time=batch_time,\n",
    "                                                                    data_time=data_time, loss=losses))\n",
    "    del lr_imgs, hr_imgs, sr_imgs  # free some memory since their histories may be stored\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
